{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchvision.utils import save_image\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output,display,HTML\n",
    "import base64\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('tranining_dataset', exist_ok=True)\n",
    "num_videos = 10000\n",
    "frames_per_video =10\n",
    "img_size =(64,64)\n",
    "shape_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts_and_movements = [\n",
    "    (\"circle moving down\", \"circle\", \"down\"),  # Move circle downward\n",
    "    (\"circle moving left\", \"circle\", \"left\"),  # Move circle leftward\n",
    "    (\"circle moving right\", \"circle\", \"right\"),  # Move circle rightward\n",
    "    (\"circle moving diagonally up-right\", \"circle\", \"diagonal_up_right\"),  # Move circle diagonally up-right\n",
    "    (\"circle moving diagonally down-left\", \"circle\", \"diagonal_down_left\"),  # Move circle diagonally down-left\n",
    "    (\"circle moving diagonally up-left\", \"circle\", \"diagonal_up_left\"),  # Move circle diagonally up-left\n",
    "    (\"circle moving diagonally down-right\", \"circle\", \"diagonal_down_right\"),  # Move circle diagonally down-right\n",
    "    (\"circle rotating clockwise\", \"circle\", \"rotate_clockwise\"),  # Rotate circle clockwise\n",
    "    (\"circle rotating counter-clockwise\", \"circle\", \"rotate_counter_clockwise\"),  # Rotate circle counter-clockwise\n",
    "    (\"circle shrinking\", \"circle\", \"shrink\"),  # Shrink circle\n",
    "    (\"circle expanding\", \"circle\", \"expand\"),  # Expand circle\n",
    "    (\"circle bouncing vertically\", \"circle\", \"bounce_vertical\"),  # Bounce circle vertically\n",
    "    (\"circle bouncing horizontally\", \"circle\", \"bounce_horizontal\"),  # Bounce circle horizontally\n",
    "    (\"circle zigzagging vertically\", \"circle\", \"zigzag_vertical\"),  # Zigzag circle vertically\n",
    "    (\"circle zigzagging horizontally\", \"circle\", \"zigzag_horizontal\"),  # Zigzag circle horizontally\n",
    "    (\"circle moving up-left\", \"circle\", \"up_left\"),  # Move circle up-left\n",
    "    (\"circle moving down-right\", \"circle\", \"down_right\"),  # Move circle down-right\n",
    "    (\"circle moving down-left\", \"circle\", \"down_left\"),  # Move circle down-left\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_image_with_moving_shape(size, frame_num, shape, direction):\n",
    "    img = Image.new('RGB',size, color=(255,255,255))\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    center_x, center_y = size[0]//2, size[1]//2\n",
    "    postion = (center_x, center_y)\n",
    "    direction_map = {  \n",
    "        # Adjust position downwards based on frame number\n",
    "        \"down\": (0, frame_num * 5 % size[1]),  \n",
    "        # Adjust position to the left based on frame number\n",
    "        \"left\": (-frame_num * 5 % size[0], 0),  \n",
    "        # Adjust position to the right based on frame number\n",
    "        \"right\": (frame_num * 5 % size[0], 0),  \n",
    "        # Adjust position diagonally up and to the right\n",
    "        \"diagonal_up_right\": (frame_num * 5 % size[0], -frame_num * 5 % size[1]),  \n",
    "        # Adjust position diagonally down and to the left\n",
    "        \"diagonal_down_left\": (-frame_num * 5 % size[0], frame_num * 5 % size[1]),  \n",
    "        # Adjust position diagonally up and to the left\n",
    "        \"diagonal_up_left\": (-frame_num * 5 % size[0], -frame_num * 5 % size[1]),  \n",
    "        # Adjust position diagonally down and to the right\n",
    "        \"diagonal_down_right\": (frame_num * 5 % size[0], frame_num * 5 % size[1]),  \n",
    "        # Rotate the image clockwise based on frame number\n",
    "        \"rotate_clockwise\": img.rotate(frame_num * 10 % 360, center=(center_x, center_y), fillcolor=(255, 255, 255)),  \n",
    "        # Rotate the image counter-clockwise based on frame number\n",
    "        \"rotate_counter_clockwise\": img.rotate(-frame_num * 10 % 360, center=(center_x, center_y), fillcolor=(255, 255, 255)),  \n",
    "        # Adjust position for a bouncing effect vertically\n",
    "        \"bounce_vertical\": (0, center_y - abs(frame_num * 5 % size[1] - center_y)),  \n",
    "        # Adjust position for a bouncing effect horizontally\n",
    "        \"bounce_horizontal\": (center_x - abs(frame_num * 5 % size[0] - center_x), 0),  \n",
    "        # Adjust position for a zigzag effect vertically\n",
    "        \"zigzag_vertical\": (0, center_y - frame_num * 5 % size[1]) if frame_num % 2 == 0 else (0, center_y + frame_num * 5 % size[1]),  \n",
    "        # Adjust position for a zigzag effect horizontally\n",
    "        \"zigzag_horizontal\": (center_x - frame_num * 5 % size[0], center_y) if frame_num % 2 == 0 else (center_x + frame_num * 5 % size[0], center_y),  \n",
    "        # Adjust position upwards and to the right based on frame number\n",
    "        \"up_right\": (frame_num * 5 % size[0], -frame_num * 5 % size[1]),  \n",
    "        # Adjust position upwards and to the left based on frame number\n",
    "        \"up_left\": (-frame_num * 5 % size[0], -frame_num * 5 % size[1]),  \n",
    "        # Adjust position downwards and to the right based on frame number\n",
    "        \"down_right\": (frame_num * 5 % size[0], frame_num * 5 % size[1]),  \n",
    "        # Adjust position downwards and to the left based on frame number\n",
    "        \"down_left\": (-frame_num * 5 % size[0], frame_num * 5 % size[1])  \n",
    "    }\n",
    "\n",
    "    if direction in direction_map:\n",
    "        if isinstance(direction_map[direction],tuple):\n",
    "            postion = tuple(np.add(postion,direction_map[direction]))\n",
    "        else:\n",
    "            img = direction_map[direction]\n",
    "\n",
    "\n",
    "    return np.array(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(num_videos):\n",
    "    prompt, shape, direction = random.choice(prompts_and_movements)\n",
    "    video_dir = f'training_dataset/video_{i}'\n",
    "    os.makedirs(video_dir,exist_ok=True)\n",
    "    with open(f'{video_dir}/prompt.txt', 'w') as f:\n",
    "        f.write(prompt)\n",
    "\n",
    "    for frame_num in range(frames_per_video):\n",
    "        img =create_image_with_moving_shape(img_size,frame_num,shape,direction)\n",
    "        cv2.imwrite(f'{video_dir}/frame_{frame_num}.png',img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Text2VideoDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.video_dirs = [os.path.join(root_dir,d) for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir,d))]\n",
    "        self.frame_paths=[]\n",
    "        self.prompts=[]\n",
    "\n",
    "        for video_dir in self.video_dirs:\n",
    "            frames =[os.path.join(video_dir,f) for f in os.listdir(video_dir) if f.endswith('.png')]\n",
    "            self.frame_paths.extend(frames)\n",
    "            with open(os.path.join(video_dir,'prompt.txt'),'r')as f:\n",
    "                prompt = f.read().strip()\n",
    "\n",
    "            self.prompts.extend([prompt] * len(frames))\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.frame_paths)\n",
    "\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        frame_path = self.frame_paths[idx]\n",
    "        image = Image.open(frame_path)\n",
    "        prompt = self.prompts[idx]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "\n",
    "        return image, prompt\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,),(0.5,))\n",
    "    ]\n",
    ")\n",
    "dataset = Text2VideoDataset(root_dir='training_dataset', transform=transform)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=16, shuffle =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super(TextEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, text_embed_size):\n",
    "        super(Generator, self).__init__()\n",
    "        self.fc1 = nn.Linear(100 + text_embed_size, 256*8*8)\n",
    "        self.deconv1 = nn.ConvTranspose2d(256,128,4,2,1)\n",
    "        self.deconv2 = nn.ConvTranspose2d(128,64,4,2,1)\n",
    "        self.deconv3 = nn.ConvTranspose2d(64,3,4,2,1)\n",
    "\n",
    "        self.relu = nn.ReLU(True)\n",
    "        self.tanh = nn.Tanh()\n",
    "    \n",
    "    def forward(self, noise, text_embed):\n",
    "        x= torch.cat((noise, text_embed), dim=1)\n",
    "        x = self.fc1(x).view(-1,256,8,8)\n",
    "        x= self.relu(self.deconv1(x))\n",
    "        x= self.relu(self.deconv2(x))\n",
    "        x= self.tanh(self.deconv3(x))\n",
    "\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3,64,4,2,1)\n",
    "        self.conv2 = nn.Conv2d(64,128,4,2,1)\n",
    "        self.conv3 = nn.Conv2d(128,256,4,2,1)\n",
    "\n",
    "        self.fc1 = nn.Linear(256*8*8, 1)\n",
    "        self.leaky_relu = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.leaky_relu(self.conv1(input))\n",
    "        x= self.leaky_relu(self.conv2(x))\n",
    "        x= self.leaky_relu(self.conv3(x))\n",
    "\n",
    "        x= x.view(-1,256*8*8)\n",
    "        x= self.sigmoid(self.fc1(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "all_prompts = [prompt for prompt, _, _ in prompts_and_movements] \n",
    "vocab = {word: idx for idx, word in enumerate(set(\" \".join(all_prompts).split()))} \n",
    "vocab_size = len(vocab)  \n",
    "embed_size = 10  \n",
    "\n",
    "def encode_text(prompt):\n",
    "    return torch.tensor([vocab[word] for word in prompt.split()])\n",
    "\n",
    "text_embedding = TextEmbedding(vocab_size, embed_size).to(device)  \n",
    "netG = Generator(embed_size).to(device)  \n",
    "netD = Discriminator().to(device)  \n",
    "criterion = nn.BCELoss().to(device)  \n",
    "optimizerD = optim.Adam(netD.parameters(), lr=0.0002, betas=(0.5, 0.999))  # Adam optimizer for Discriminator\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=0.0002, betas=(0.5, 0.999))  # Adam optimizer for Genera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of epochs\n",
    "num_epochs = 13\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (data, prompts) in enumerate(dataloader):\n",
    "        real_data = data.to(device)\n",
    "        prompts = [prompt for prompt in prompts]\n",
    "        netD.zero_grad()  \n",
    "        batch_size = real_data.size(0)  \n",
    "        labels = torch.ones(batch_size, 1).to(device)  \n",
    "        output = netD(real_data)  \n",
    "        lossD_real = criterion(output, labels)  \n",
    "        lossD_real.backward()  \n",
    "       \n",
    "        # Generate fake data\n",
    "        noise = torch.randn(batch_size, 100).to(device)  \n",
    "        text_embeds = torch.stack([text_embedding(encode_text(prompt).to(device)).mean(dim=0) for prompt in prompts])  \n",
    "        fake_data = netG(noise, text_embeds)  \n",
    "        labels = torch.zeros(batch_size, 1).to(device) \n",
    "        output = netD(fake_data.detach())  \n",
    "        lossD_fake = criterion(output, labels)  \n",
    "        lossD_fake.backward()  \n",
    "        optimizerD.step()  \n",
    "\n",
    "        # Update Generator\n",
    "        netG.zero_grad()  \n",
    "        labels = torch.ones(batch_size, 1).to(device) \n",
    "        output = netD(fake_data) \n",
    "        lossG = criterion(output, labels)  \n",
    "        lossG.backward()  \n",
    "        optimizerG.step()  \n",
    "    \n",
    "    # Print epoch information\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}] Loss D: {lossD_real + lossD_fake}, Loss G: {lossG}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Generator model's state dictionary to a file named 'generator.pth'\n",
    "torch.save(netG.state_dict(), 'generator.pth')\n",
    "\n",
    "# Save the Discriminator model's state dictionary to a file named 'discriminator.pth'\n",
    "torch.save(netD.state_dict(), 'discriminator.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_video(text_prompt, num_frames=10):\n",
    "    os.makedirs(f'generated_video_{text_prompt.replace(\" \", \"_\")}', exist_ok=True)\n",
    "    text_embed = text_embedding(encode_text(text_prompt).to(device)).mean(dim=0).unsqueeze(0)\n",
    "    for frame_num in range(num_frames):\n",
    "        noise = torch.randn(1, 100).to(device)\n",
    "        with torch.no_grad():\n",
    "            fake_frame = netG(noise, text_embed)\n",
    "        save_image(fake_frame, f'generated_video_{text_prompt.replace(\" \", \"_\")}/frame_{frame_num}.png')\n",
    "generate_video('circle moving up-right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "folder_path = 'generated_video_circle_moving_up-right'\n",
    "image_files = [f for f in os.listdir(folder_path) if f.endswith('.png')]\n",
    "image_files.sort()\n",
    "frames = []\n",
    "for image_file in image_files:\n",
    "  image_path = os.path.join(folder_path, image_file)\n",
    "  frame = cv2.imread(image_path)\n",
    "  frames.append(frame)\n",
    "\n",
    "\n",
    "frames = np.array(frames)\n",
    "\n",
    "# Define the frame rate (frames per second)\n",
    "fps = 10\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter('generated_video.avi', fourcc, fps, (frames[0].shape[1], frames[0].shape[0]))\n",
    "\n",
    "# Write each frame to the video\n",
    "for frame in frames:\n",
    "  out.write(frame)\n",
    "\n",
    "# Release the video writer\n",
    "out.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
